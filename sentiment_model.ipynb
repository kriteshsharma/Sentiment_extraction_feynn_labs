{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tweet Pre Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1e5decab-0213-4bcb-af16-6e66845378d0",
    "_uuid": "24b0baf5-6272-459c-b65d-e1b8d7cd61e0",
    "execution": {
     "iopub.execute_input": "2021-11-21T04:44:49.213044Z",
     "iopub.status.busy": "2021-11-21T04:44:49.212456Z",
     "iopub.status.idle": "2021-11-21T04:44:50.44074Z",
     "shell.execute_reply": "2021-11-21T04:44:50.439968Z",
     "shell.execute_reply.started": "2021-11-21T04:44:49.213001Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "import string\n",
    "import pkg_resources\n",
    "\n",
    "def replaceElongated(word):\n",
    "    \"\"\" Replaces an elongated word with its basic form, unless the word exists in the lexicon \"\"\"\n",
    "\n",
    "    repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    repl = r'\\1\\2\\3'\n",
    "    if wordnet.synsets(word):\n",
    "        return word\n",
    "    repl_word = repeat_regexp.sub(repl, word)\n",
    "    if repl_word != word:      \n",
    "        return replaceElongated(repl_word)\n",
    "    else:       \n",
    "        return repl_word\n",
    "    \n",
    "def load_dict_contractions():    \n",
    "    return {\n",
    "        \"cant\":\"can not\",\n",
    "        \"dont\":\"do not\",\n",
    "        \"wont\":\"will not\",\n",
    "        \"ain't\":\"is not\",\n",
    "        \"amn't\":\"am not\",\n",
    "        \"aren't\":\"are not\",\n",
    "        \"can't\":\"cannot\",\n",
    "        \"'cause\":\"because\",\n",
    "        \"couldn't\":\"could not\",\n",
    "        \"couldn't've\":\"could not have\",\n",
    "        \"could've\":\"could have\",\n",
    "        \"daren't\":\"dare not\",\n",
    "        \"daresn't\":\"dare not\",\n",
    "        \"dasn't\":\"dare not\",\n",
    "        \"didn't\":\"did not\",\n",
    "        \"doesn't\":\"does not\",\n",
    "        \"don't\":\"do not\",\n",
    "        \"e'er\":\"ever\",\n",
    "        \"em\":\"them\",\n",
    "        \"everyone's\":\"everyone is\",\n",
    "        \"finna\":\"fixing to\",\n",
    "        \"gimme\":\"give me\",\n",
    "        \"gonna\":\"going to\",\n",
    "        \"gon't\":\"go not\",\n",
    "        \"gotta\":\"got to\",\n",
    "        \"hadn't\":\"had not\",\n",
    "        \"hasn't\":\"has not\",\n",
    "        \"haven't\":\"have not\",\n",
    "        \"he'd\":\"he would\",\n",
    "        \"he'll\":\"he will\",\n",
    "        \"he's\":\"he is\",\n",
    "        \"he've\":\"he have\",\n",
    "        \"how'd\":\"how would\",\n",
    "        \"how'll\":\"how will\",\n",
    "        \"how're\":\"how are\",\n",
    "        \"how's\":\"how is\",\n",
    "        \"I'd\":\"I would\",\n",
    "        \"I'll\":\"I will\",\n",
    "        \"I'm\":\"I am\",\n",
    "        \"I'm'a\":\"I am about to\",\n",
    "        \"I'm'o\":\"I am going to\",\n",
    "        \"isn't\":\"is not\",\n",
    "        \"it'd\":\"it would\",\n",
    "        \"it'll\":\"it will\",\n",
    "        \"it's\":\"it is\",\n",
    "        \"I've\":\"I have\",\n",
    "        \"kinda\":\"kind of\",\n",
    "        \"let's\":\"let us\",\n",
    "        \"mayn't\":\"may not\",\n",
    "        \"may've\":\"may have\",\n",
    "        \"mightn't\":\"might not\",\n",
    "        \"might've\":\"might have\",\n",
    "        \"mustn't\":\"must not\",\n",
    "        \"mustn't've\":\"must not have\",\n",
    "        \"must've\":\"must have\",\n",
    "        \"needn't\":\"need not\",\n",
    "        \"ne'er\":\"never\",\n",
    "        \"o'\":\"of\",\n",
    "        \"o'er\":\"over\",\n",
    "        \"ol'\":\"old\",\n",
    "        \"oughtn't\":\"ought not\",\n",
    "        \"shalln't\":\"shall not\",\n",
    "        \"shan't\":\"shall not\",\n",
    "        \"she'd\":\"she would\",\n",
    "        \"she'll\":\"she will\",\n",
    "        \"she's\":\"she is\",\n",
    "        \"shouldn't\":\"should not\",\n",
    "        \"shouldn't've\":\"should not have\",\n",
    "        \"should've\":\"should have\",\n",
    "        \"somebody's\":\"somebody is\",\n",
    "        \"someone's\":\"someone is\",\n",
    "        \"something's\":\"something is\",\n",
    "        \"that'd\":\"that would\",\n",
    "        \"that'll\":\"that will\",\n",
    "        \"that're\":\"that are\",\n",
    "        \"that's\":\"that is\",\n",
    "        \"there'd\":\"there would\",\n",
    "        \"there'll\":\"there will\",\n",
    "        \"there're\":\"there are\",\n",
    "        \"there's\":\"there is\",\n",
    "        \"these're\":\"these are\",\n",
    "        \"they'd\":\"they would\",\n",
    "        \"they'll\":\"they will\",\n",
    "        \"they're\":\"they are\",\n",
    "        \"they've\":\"they have\",\n",
    "        \"this's\":\"this is\",\n",
    "        \"those're\":\"those are\",\n",
    "        \"'tis\":\"it is\",\n",
    "        \"'twas\":\"it was\",\n",
    "        \"wanna\":\"want to\",\n",
    "        \"wasn't\":\"was not\",\n",
    "        \"we'd\":\"we would\",\n",
    "        \"we'd've\":\"we would have\",\n",
    "        \"we'll\":\"we will\",\n",
    "        \"we're\":\"we are\",\n",
    "        \"weren't\":\"were not\",\n",
    "        \"we've\":\"we have\",\n",
    "        \"what'd\":\"what did\",\n",
    "        \"what'll\":\"what will\",\n",
    "        \"what're\":\"what are\",\n",
    "        \"what's\":\"what is\",\n",
    "        \"what've\":\"what have\",\n",
    "        \"when's\":\"when is\",\n",
    "        \"where'd\":\"where did\",\n",
    "        \"where're\":\"where are\",\n",
    "        \"where's\":\"where is\",\n",
    "        \"where've\":\"where have\",\n",
    "        \"which's\":\"which is\",\n",
    "        \"who'd\":\"who would\",\n",
    "        \"who'd've\":\"who would have\",\n",
    "        \"who'll\":\"who will\",\n",
    "        \"who're\":\"who are\",\n",
    "        \"who's\":\"who is\",\n",
    "        \"who've\":\"who have\",\n",
    "        \"why'd\":\"why did\",\n",
    "        \"why're\":\"why are\",\n",
    "        \"why's\":\"why is\",\n",
    "        \"won't\":\"will not\",\n",
    "        \"wouldn't\":\"would not\",\n",
    "        \"would've\":\"would have\",\n",
    "        \"y'all\":\"you all\",\n",
    "        \"you'd\":\"you would\",\n",
    "        \"you'll\":\"you will\",\n",
    "        \"you're\":\"you are\",\n",
    "        \"you've\":\"you have\",\n",
    "        \"Whatcha\":\"What are you\",\n",
    "        \"luv\":\"love\",\n",
    "        \"sux\":\"sucks\",\n",
    "        \"couldn't\":\"could not\",\n",
    "        \"wouldn't\":\"would not\",\n",
    "        \"shouldn't\":\"should not\",\n",
    "        \"im\":\"i am\"\n",
    "        }\n",
    "\n",
    "single_word = list(string.ascii_lowercase)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))-set(['not', 'no'])\n",
    "\n",
    "def normalization(text):\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Unicodes\n",
    "    text = re.sub(r'(\\\\u[0-9A-Fa-f]+)',r'', text)       \n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r'',text)\n",
    "    \n",
    "    # URL\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',text)\n",
    "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "    \n",
    "    # User Tag\n",
    "    text = re.sub('@[^\\s]+',' ',text)\n",
    "    \n",
    "    # Hash Tag\n",
    "    text = re.sub(r'#([^\\s]+)', r' ', text)\n",
    "    \n",
    "    # Number\n",
    "    text = ''.join([i for i in text if not i.isdigit()])      \n",
    "    \n",
    "    \n",
    "    # Punctuation\n",
    "    #text = ' '.join([char for char in text if char not in string.punctuation])\n",
    "    for sym in string.punctuation:\n",
    "        text = text.replace(sym, \" \")\n",
    "    \n",
    "    # Elongated Words\n",
    "    for word in text.split():\n",
    "        text = text.replace(word, replaceElongated(word))\n",
    "    \n",
    "    # Contraction\n",
    "    CONTRACTIONS = load_dict_contractions()\n",
    "    text = text.replace(\"â€™\",\"'\")\n",
    "    words = text.split()\n",
    "    reformed = [CONTRACTIONS[word] if word in CONTRACTIONS else word for word in words]\n",
    "    text = \" \".join(reformed)\n",
    "    # Single Character   \n",
    "    text = ' '.join( [w for w in text.split() if len(w)>1 and w != 'a' and w != 'i'])\n",
    "         \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T04:44:50.44303Z",
     "iopub.status.busy": "2021-11-21T04:44:50.442684Z",
     "iopub.status.idle": "2021-11-21T04:44:50.452557Z",
     "shell.execute_reply": "2021-11-21T04:44:50.451728Z",
     "shell.execute_reply.started": "2021-11-21T04:44:50.442992Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T04:44:50.4543Z",
     "iopub.status.busy": "2021-11-21T04:44:50.453778Z",
     "iopub.status.idle": "2021-11-21T04:44:50.875289Z",
     "shell.execute_reply": "2021-11-21T04:44:50.874509Z",
     "shell.execute_reply.started": "2021-11-21T04:44:50.454258Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train=pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n",
    "df_train_2=pd.read_csv('../input/complete-tweet-sentiment-extraction-data/tweet_dataset.csv')\n",
    "df_test=pd.read_csv('../input/tweet-sentiment-extraction/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Label Encoding and conversion into categorial vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T04:44:50.950992Z",
     "iopub.status.busy": "2021-11-21T04:44:50.950663Z",
     "iopub.status.idle": "2021-11-21T04:44:50.955228Z",
     "shell.execute_reply": "2021-11-21T04:44:50.954094Z",
     "shell.execute_reply.started": "2021-11-21T04:44:50.950963Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T04:44:50.959317Z",
     "iopub.status.busy": "2021-11-21T04:44:50.958686Z",
     "iopub.status.idle": "2021-11-21T04:44:51.025847Z",
     "shell.execute_reply": "2021-11-21T04:44:51.025023Z",
     "shell.execute_reply.started": "2021-11-21T04:44:50.959268Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_2.info()\n",
    "df_train_2.drop(columns=['sentiment', 'author', 'old_text', 'aux_id'], inplace=True)\n",
    "df_train_2=df_train_2.rename(columns={'new_sentiment':'sentiment'})\n",
    "df_train_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T04:44:51.028158Z",
     "iopub.status.busy": "2021-11-21T04:44:51.02778Z",
     "iopub.status.idle": "2021-11-21T04:44:51.096744Z",
     "shell.execute_reply": "2021-11-21T04:44:51.095883Z",
     "shell.execute_reply.started": "2021-11-21T04:44:51.028121Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.info()\n",
    "df_train=df_train[['textID', 'text', 'selected_text', 'sentiment']]\n",
    "df_train_2.info()\n",
    "\n",
    "df_train=df_train.append(df_train_2)\n",
    "df_train['sentiment'].replace('', np.nan, inplace=True)\n",
    "df_train.dropna(subset=['sentiment'], inplace=True)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T04:44:51.098526Z",
     "iopub.status.busy": "2021-11-21T04:44:51.098012Z",
     "iopub.status.idle": "2021-11-21T04:44:51.109133Z",
     "shell.execute_reply": "2021-11-21T04:44:51.107819Z",
     "shell.execute_reply.started": "2021-11-21T04:44:51.098486Z"
    }
   },
   "outputs": [],
   "source": [
    "print(df_train['sentiment'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T04:44:51.112344Z",
     "iopub.status.busy": "2021-11-21T04:44:51.11182Z",
     "iopub.status.idle": "2021-11-21T04:44:51.164541Z",
     "shell.execute_reply": "2021-11-21T04:44:51.16365Z",
     "shell.execute_reply.started": "2021-11-21T04:44:51.112296Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train['encoded_sentiment']=encoder.fit_transform(df_train['sentiment'])\n",
    "df_train=pd.get_dummies(df_train, columns=['sentiment'])\n",
    "df_train=df_train[['textID', 'text', 'encoded_sentiment', 'sentiment_negative', 'sentiment_neutral', 'sentiment_positive', 'selected_text']]\n",
    "#df_train=df_train[['textID', 'text', 'encoded_sentiment', 'sentiment_negative', 'sentiment_positive', 'selected_text']]\n",
    "\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T04:44:51.166057Z",
     "iopub.status.busy": "2021-11-21T04:44:51.165729Z",
     "iopub.status.idle": "2021-11-21T04:44:51.190959Z",
     "shell.execute_reply": "2021-11-21T04:44:51.190261Z",
     "shell.execute_reply.started": "2021-11-21T04:44:51.166022Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test['encoded_sentiment']=encoder.fit_transform(df_test['sentiment'])\n",
    "df_test=pd.get_dummies(df_test, columns=['sentiment'])\n",
    "df_test=df_test[['textID', 'text', 'encoded_sentiment', 'sentiment_negative', 'sentiment_neutral', 'sentiment_positive']]\n",
    "#df_test=df_test[['textID', 'text', 'encoded_sentiment', 'sentiment_negative', 'sentiment_positive']]\n",
    "\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T04:44:51.194395Z",
     "iopub.status.busy": "2021-11-21T04:44:51.194101Z",
     "iopub.status.idle": "2021-11-21T04:45:16.037888Z",
     "shell.execute_reply": "2021-11-21T04:45:16.037163Z",
     "shell.execute_reply.started": "2021-11-21T04:44:51.194369Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['text'].apply(normalization)\n",
    "df_train['text'].replace('', np.nan, inplace=True)\n",
    "df_train.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T04:45:16.039798Z",
     "iopub.status.busy": "2021-11-21T04:45:16.039428Z",
     "iopub.status.idle": "2021-11-21T04:45:17.144953Z",
     "shell.execute_reply": "2021-11-21T04:45:17.144275Z",
     "shell.execute_reply.started": "2021-11-21T04:45:16.039758Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test['text'] = df_test['text'].apply(normalization)\n",
    "df_test['text'].replace('', np.nan, inplace=True)\n",
    "df_test.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T04:45:17.146665Z",
     "iopub.status.busy": "2021-11-21T04:45:17.146341Z",
     "iopub.status.idle": "2021-11-21T04:45:17.157138Z",
     "shell.execute_reply": "2021-11-21T04:45:17.15622Z",
     "shell.execute_reply.started": "2021-11-21T04:45:17.14663Z"
    }
   },
   "outputs": [],
   "source": [
    "#pos = len(df['encoded_sentiment'][df.encoded_sentiment == 2])\n",
    "pos = len(df_train['encoded_sentiment'][df_train.encoded_sentiment == 2])\n",
    "neu = len(df_train['encoded_sentiment'][df_train.encoded_sentiment == 1])\n",
    "neg = len(df_train['encoded_sentiment'][df_train.encoded_sentiment == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T04:45:17.159096Z",
     "iopub.status.busy": "2021-11-21T04:45:17.158693Z",
     "iopub.status.idle": "2021-11-21T04:45:17.259262Z",
     "shell.execute_reply": "2021-11-21T04:45:17.258507Z",
     "shell.execute_reply.started": "2021-11-21T04:45:17.159058Z"
    }
   },
   "outputs": [],
   "source": [
    "def word_count(sentence):\n",
    "    return len(str(sentence).split())\n",
    "df_train['word_count'] = df_train['text'].apply(word_count)\n",
    "\n",
    "#pos_sen_len = df['word_count'][df.encoded_sentiment == 2]\n",
    "pos_sen_len = df_train['word_count'][df_train.encoded_sentiment == 2]\n",
    "neu_sen_len = df_train['word_count'][df_train.encoded_sentiment == 1]\n",
    "neg_sen_len = df_train['word_count'][df_train.encoded_sentiment == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Plotting distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T04:45:17.261314Z",
     "iopub.status.busy": "2021-11-21T04:45:17.260937Z",
     "iopub.status.idle": "2021-11-21T04:45:17.527714Z",
     "shell.execute_reply": "2021-11-21T04:45:17.527Z",
     "shell.execute_reply.started": "2021-11-21T04:45:17.261268Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.xlim(0, 35, 5)\n",
    "plt.xlabel('word count')\n",
    "plt.ylabel('frequency')\n",
    "plt.hist([pos_sen_len, neu_sen_len, neg_sen_len], color=['r', 'g', 'b'], alpha=0.5, label=['positive', 'neutral', 'negative'])\n",
    "#plt.hist([pos_sen_len, neg_sen_len], color=['r', 'b'], alpha=0.5, label=['positive', 'negative'])\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T04:45:17.5312Z",
     "iopub.status.busy": "2021-11-21T04:45:17.530914Z",
     "iopub.status.idle": "2021-11-21T04:45:17.874251Z",
     "shell.execute_reply": "2021-11-21T04:45:17.873218Z",
     "shell.execute_reply.started": "2021-11-21T04:45:17.531173Z"
    }
   },
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for line in list(df_train['text']):\n",
    "    words = str(line).split()\n",
    "    for word in words:\n",
    "        all_words.append(word.lower())     \n",
    "print(Counter(all_words).most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T04:45:17.876404Z",
     "iopub.status.busy": "2021-11-21T04:45:17.875792Z",
     "iopub.status.idle": "2021-11-21T04:45:17.907093Z",
     "shell.execute_reply": "2021-11-21T04:45:17.906012Z",
     "shell.execute_reply.started": "2021-11-21T04:45:17.87636Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.info()\n",
    "#X=(df_train.iloc[:, 1].values).astype('U')\n",
    "y=(df_train.iloc[:, 3:6].values).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T04:45:17.909512Z",
     "iopub.status.busy": "2021-11-21T04:45:17.909112Z",
     "iopub.status.idle": "2021-11-21T04:45:17.923184Z",
     "shell.execute_reply": "2021-11-21T04:45:17.921899Z",
     "shell.execute_reply.started": "2021-11-21T04:45:17.909474Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test.info()\n",
    "#X_test=(df_test.iloc[:, 1].values).astype('U')\n",
    "y_test=(df_test.iloc[:, 3:6].values).astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Entire dataset shuffling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T04:45:17.925076Z",
     "iopub.status.busy": "2021-11-21T04:45:17.924695Z",
     "iopub.status.idle": "2021-11-21T04:45:17.931443Z",
     "shell.execute_reply": "2021-11-21T04:45:17.930547Z",
     "shell.execute_reply.started": "2021-11-21T04:45:17.925037Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.utils import shuffle\n",
    "X, y = shuffle(X, y, random_state=0)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Createing Glove Tweet Vector Dictonary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T04:45:17.933616Z",
     "iopub.status.busy": "2021-11-21T04:45:17.932903Z",
     "iopub.status.idle": "2021-11-21T04:50:15.773691Z",
     "shell.execute_reply": "2021-11-21T04:50:15.772693Z",
     "shell.execute_reply.started": "2021-11-21T04:45:17.933578Z"
    }
   },
   "outputs": [],
   "source": [
    "# Glove Word Vocab\n",
    "from tqdm import tqdm\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words=set()\n",
    "        word_to_vec_map=dict()\n",
    "        for line in tqdm(f):\n",
    "            line=line.strip().split()\n",
    "            curr_word=''.join(line[:-300])\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word]=np.array(line[-300:], dtype=np.float32)\n",
    "            \n",
    "        i=1\n",
    "        words_to_index=dict()\n",
    "        index_to_words=dict()\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w]=i\n",
    "            index_to_words[i]=w\n",
    "            i+=1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "word_to_index, index_to_word, word_to_vec_map=read_glove_vecs('../input/glove840b/glove.840B.300d.txt')\n",
    "\n",
    "print(len(word_to_index))\n",
    "print(list(word_to_index.items())[:5])\n",
    "print(list(word_to_vec_map.items())[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Keras Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T04:50:15.775924Z",
     "iopub.status.busy": "2021-11-21T04:50:15.775339Z",
     "iopub.status.idle": "2021-11-21T04:50:19.893184Z",
     "shell.execute_reply": "2021-11-21T04:50:19.892351Z",
     "shell.execute_reply.started": "2021-11-21T04:50:15.775882Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "from keras import regularizers, callbacks\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation, Bidirectional, SpatialDropout1D, GlobalMaxPooling1D, BatchNormalization, PReLU\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import RMSprop, Adam, Adamax, SGD\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.losses import BinaryCrossentropy\n",
    "from keras.metrics import Precision, Recall, CategoricalAccuracy\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Counting the words which have not been used in word embeddidng**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T04:50:19.904604Z",
     "iopub.status.busy": "2021-11-21T04:50:19.903522Z",
     "iopub.status.idle": "2021-11-21T04:50:19.929518Z",
     "shell.execute_reply": "2021-11-21T04:50:19.928743Z",
     "shell.execute_reply.started": "2021-11-21T04:50:19.904557Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "tk=text.Tokenizer(num_words=200000)\n",
    "\n",
    "df_train.text=df_train.text.astype(str)\n",
    "df_test.text=df_test.text.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T04:50:19.93159Z",
     "iopub.status.busy": "2021-11-21T04:50:19.931213Z",
     "iopub.status.idle": "2021-11-21T04:50:22.608352Z",
     "shell.execute_reply": "2021-11-21T04:50:22.60709Z",
     "shell.execute_reply.started": "2021-11-21T04:50:19.931549Z"
    }
   },
   "outputs": [],
   "source": [
    "tk.fit_on_texts(list(df_train.text.values)+list(df_test.text.values))\n",
    "X_text_indices=tk.texts_to_sequences(df_train.text.values)\n",
    "X_test_text_indices=tk.texts_to_sequences(df_test.text.values)\n",
    "\n",
    "maxlen=-1\n",
    "for text in X_text_indices:\n",
    "    if len(text)>maxlen:\n",
    "        maxlen=len(text)\n",
    "for text in X_test_text_indices:\n",
    "    if len(text)>maxlen:\n",
    "        maxlen=len(text)       \n",
    "print(maxlen)\n",
    "\n",
    "X_text_indices=sequence.pad_sequences(X_text_indices, maxlen=maxlen)\n",
    "print(X_text_indices.shape)\n",
    "\n",
    "X_test_text_indices=sequence.pad_sequences(X_test_text_indices, maxlen=maxlen)\n",
    "print(X_test_text_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T04:50:22.610034Z",
     "iopub.status.busy": "2021-11-21T04:50:22.609699Z",
     "iopub.status.idle": "2021-11-21T04:50:22.618542Z",
     "shell.execute_reply": "2021-11-21T04:50:22.61772Z",
     "shell.execute_reply.started": "2021-11-21T04:50:22.609996Z"
    }
   },
   "outputs": [],
   "source": [
    "word_index=tk.word_index\n",
    "print(len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Custom Word Embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T04:50:22.620493Z",
     "iopub.status.busy": "2021-11-21T04:50:22.620089Z",
     "iopub.status.idle": "2021-11-21T04:50:22.728195Z",
     "shell.execute_reply": "2021-11-21T04:50:22.727202Z",
     "shell.execute_reply.started": "2021-11-21T04:50:22.620453Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_matrix=np.zeros((len(word_index)+1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector=word_to_vec_map.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i]=embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T04:50:22.768827Z",
     "iopub.status.busy": "2021-11-21T04:50:22.768067Z",
     "iopub.status.idle": "2021-11-21T04:50:25.769058Z",
     "shell.execute_reply": "2021-11-21T04:50:25.76832Z",
     "shell.execute_reply.started": "2021-11-21T04:50:22.768779Z"
    }
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(len(word_index)+1, 300, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
    "\n",
    "model.add(LSTM(units=300, return_sequences=False, recurrent_dropout=0.2, dropout=0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(200))\n",
    "model.add(PReLU())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(200))\n",
    "model.add(PReLU())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(200))\n",
    "model.add(PReLU())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(200))\n",
    "model.add(PReLU())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(units=3))\n",
    "model.add(Activation(activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T04:50:25.775521Z",
     "iopub.status.busy": "2021-11-21T04:50:25.775265Z",
     "iopub.status.idle": "2021-11-21T04:50:25.786067Z",
     "shell.execute_reply": "2021-11-21T04:50:25.78527Z",
     "shell.execute_reply.started": "2021-11-21T04:50:25.775494Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T04:50:25.791254Z",
     "iopub.status.busy": "2021-11-21T04:50:25.790921Z",
     "iopub.status.idle": "2021-11-21T04:50:25.839961Z",
     "shell.execute_reply": "2021-11-21T04:50:25.839325Z",
     "shell.execute_reply.started": "2021-11-21T04:50:25.791227Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy' , optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T04:50:25.8418Z",
     "iopub.status.busy": "2021-11-21T04:50:25.841258Z",
     "iopub.status.idle": "2021-11-21T04:50:25.857587Z",
     "shell.execute_reply": "2021-11-21T04:50:25.856798Z",
     "shell.execute_reply.started": "2021-11-21T04:50:25.841763Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_text_indices, X_dev_text_indices, y_train, y_dev = train_test_split(X_text_indices, y, test_size=0.1, random_state=42)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T04:50:25.860537Z",
     "iopub.status.busy": "2021-11-21T04:50:25.860292Z",
     "iopub.status.idle": "2021-11-21T05:02:20.059225Z",
     "shell.execute_reply": "2021-11-21T05:02:20.058419Z",
     "shell.execute_reply.started": "2021-11-21T04:50:25.860513Z"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train_text_indices, y_train, epochs=50, batch_size=128, validation_data=(X_dev_text_indices, y_dev), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loss vs epooch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T05:02:20.061039Z",
     "iopub.status.busy": "2021-11-21T05:02:20.060707Z",
     "iopub.status.idle": "2021-11-21T05:02:20.212423Z",
     "shell.execute_reply": "2021-11-21T05:02:20.211585Z",
     "shell.execute_reply.started": "2021-11-21T05:02:20.061003Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T05:02:20.214329Z",
     "iopub.status.busy": "2021-11-21T05:02:20.213751Z",
     "iopub.status.idle": "2021-11-21T05:02:20.367945Z",
     "shell.execute_reply": "2021-11-21T05:02:20.367029Z",
     "shell.execute_reply.started": "2021-11-21T05:02:20.214288Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model train vs validation accuracy')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T05:02:20.369938Z",
     "iopub.status.busy": "2021-11-21T05:02:20.369284Z",
     "iopub.status.idle": "2021-11-21T05:02:20.543509Z",
     "shell.execute_reply": "2021-11-21T05:02:20.542719Z",
     "shell.execute_reply.started": "2021-11-21T05:02:20.36989Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save('tweet_sentiment_extraction.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T05:02:20.54553Z",
     "iopub.status.busy": "2021-11-21T05:02:20.545146Z",
     "iopub.status.idle": "2021-11-21T05:02:23.658839Z",
     "shell.execute_reply": "2021-11-21T05:02:23.65803Z",
     "shell.execute_reply.started": "2021-11-21T05:02:20.545481Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('tweet_sentiment_extraction.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T05:02:23.660359Z",
     "iopub.status.busy": "2021-11-21T05:02:23.66Z",
     "iopub.status.idle": "2021-11-21T05:02:25.4627Z",
     "shell.execute_reply": "2021-11-21T05:02:25.461873Z",
     "shell.execute_reply.started": "2021-11-21T05:02:23.660316Z"
    }
   },
   "outputs": [],
   "source": [
    "para = model.evaluate(X_dev_text_indices, y_dev)\n",
    "print()\n",
    "print(\"Test loss :\", para[0], 'Test accuracy :', para[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T05:02:25.465454Z",
     "iopub.status.busy": "2021-11-21T05:02:25.465064Z",
     "iopub.status.idle": "2021-11-21T05:02:27.159891Z",
     "shell.execute_reply": "2021-11-21T05:02:27.15915Z",
     "shell.execute_reply.started": "2021-11-21T05:02:25.465424Z"
    }
   },
   "outputs": [],
   "source": [
    "y_dev_pred = model.predict(X_dev_text_indices)\n",
    "\n",
    "for i in range(len(y_dev_pred)):\n",
    "    y_dev_pred[i] = np.argmax(y_dev_pred[i])\n",
    "\n",
    "y_dev_pred = y_dev_pred[:, 0]\n",
    "print(y_dev_pred)\n",
    "\n",
    "for i in range(len(y_dev)):\n",
    "    y_dev[i] = np.argmax(y_dev[i])\n",
    "y_dev = y_dev[:, 0]\n",
    "print(y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T05:02:27.162445Z",
     "iopub.status.busy": "2021-11-21T05:02:27.16206Z",
     "iopub.status.idle": "2021-11-21T05:02:27.205695Z",
     "shell.execute_reply": "2021-11-21T05:02:27.204982Z",
     "shell.execute_reply.started": "2021-11-21T05:02:27.162405Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm_dev = confusion_matrix(y_dev, y_dev_pred)\n",
    "print(cm_dev)\n",
    "\n",
    "total=sum(sum(cm_dev))\n",
    "\n",
    "accuracy = (cm_dev[0,0]+cm_dev[1,1]+cm_dev[2,2])/total\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "sensitivity = cm_dev[0,0]/(cm_dev[0,0]+cm_dev[1,0])\n",
    "print('Sensitivity : ', sensitivity )\n",
    "\n",
    "specificity = cm_dev[1,1]/(cm_dev[1,1]+cm_dev[0,1])\n",
    "print('Specificity : ', specificity)\n",
    "\"\"\"\n",
    "accuracy = (cm_dev[0,0]+cm_dev[1,1])/total\n",
    "print('Accuracy:', accuracy)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T05:02:27.207319Z",
     "iopub.status.busy": "2021-11-21T05:02:27.20698Z",
     "iopub.status.idle": "2021-11-21T05:02:27.915549Z",
     "shell.execute_reply": "2021-11-21T05:02:27.914681Z",
     "shell.execute_reply.started": "2021-11-21T05:02:27.207284Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test_text_indices)\n",
    "\n",
    "for i in range(len(y_test_pred)):\n",
    "    y_test_pred[i] = np.argmax(y_test_pred[i])\n",
    "y_test_pred = y_test_pred[:, 0]\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    y_test[i] = np.argmax(y_test[i])\n",
    "y_test = y_test[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T05:02:27.917335Z",
     "iopub.status.busy": "2021-11-21T05:02:27.916969Z",
     "iopub.status.idle": "2021-11-21T05:02:27.948206Z",
     "shell.execute_reply": "2021-11-21T05:02:27.947002Z",
     "shell.execute_reply.started": "2021-11-21T05:02:27.917297Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm_test = confusion_matrix(y_test, y_test_pred)\n",
    "print(cm_test)\n",
    "\n",
    "total=np.sum(cm_test)\n",
    "\n",
    "accuracy = (cm_test[0,0]+cm_test[1,1]+cm_test[2,2])/total\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "sensitivity = cm_test[0,0]/(cm_test[0,0]+cm_test[1,0])\n",
    "print('Sensitivity : ', sensitivity )\n",
    "\n",
    "specificity = cm_test[1,1]/(cm_test[1,1]+cm_test[0,1])\n",
    "print('Specificity : ', specificity)\n",
    "\"\"\"\n",
    "accuracy = (cm_test[0,0]+cm_test[1,1])/total\n",
    "print('Accuracy:', accuracy)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T05:02:27.950548Z",
     "iopub.status.busy": "2021-11-21T05:02:27.950008Z",
     "iopub.status.idle": "2021-11-21T05:02:27.964143Z",
     "shell.execute_reply": "2021-11-21T05:02:27.963036Z",
     "shell.execute_reply.started": "2021-11-21T05:02:27.950507Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def jaccard_distance(y_true, y_pred, smooth=100):\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return (1 - jac) * smooth, intersection, sum_, jac\n",
    "score, intersection, sum_, jac = jaccard_distance(y_true=y_test.astype('int32'), y_pred=y_test_pred.astype('int32'))\n",
    "print(jac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-21T05:02:27.966726Z",
     "iopub.status.busy": "2021-11-21T05:02:27.966188Z",
     "iopub.status.idle": "2021-11-21T05:02:27.986924Z",
     "shell.execute_reply": "2021-11-21T05:02:27.985852Z",
     "shell.execute_reply.started": "2021-11-21T05:02:27.96667Z"
    }
   },
   "outputs": [],
   "source": [
    "#X_test_text_indices\n",
    "#X_text_indices\n",
    "df_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
